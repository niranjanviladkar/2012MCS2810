\chapter{Classification using video features}

\label{ch2_STIP}

%Replace \lipsum with text.
% You may have as many sections as you please. This is just for reference.

Video clips representing human actions can be classified based on features extracted from the video.
Below sections explain what these features are and how to use them.

\section{Spatial Temporal Interest Points\index{Spatial Temporal Interest Points}}
As explained in \cite{laptev2005} spatial temporal interest points (\index{STIP}STIPs) are 3 dimensional points in 
space and time where the local video features show significant variations. 
In \cite{actionsInContext} the features extracted at STIPs are shown to perform satisfactorily for human activity recognition. 
At each STIP, \index{Histograms of Oriented Gradient}histograms of oriented gradient (HoG\index{HoG}) and \index{Histograms of Optical Flow} histograms of optical flow (\index{HoF}HoF) are evaluated.
The number of HoG and HoF features at each STIP are 72 and 90 respectively.
Concatenating vectors of HoG and HoF features, we get a single 162 element descriptor for each STIP. 

\section{Clustering}
After extraction of STIP features, each video clip roughly has $O(1000)$ such descriptors.
100,000 descriptors are sampled randomly from all the descriptors across all the clips.
While doing random sampling, each clip is given equal chance - this avoids biased sampling
towards ( or against) any particular class of videos if they happen to be longer ( or shorter) than other videos on an average.
These 100,000 random descriptors are clustered into $k = 200$ clusters using k-means.

\section{\index{Bag of Features Representation}Bag of Features Representation}
Each descriptor in each clip is clustered into one of the $k$ clusters using
least Euclidean distance. Thus each clip is represented by a $k$ sized vector
where $i^{th}$ element in this vector represents number of descriptors of that clip 
nearest to the $i^{th}$ cluster. This vector is called a Bag of Features (\index{BoF}BoF)
representation of the clip.


\begin{table}[t]
\centering
\begin{tabular}{| l | c |}
\hline
{\bf Activity Class} & {\bf Average Precision} \\ \hline
%
AnswerPhone & 11.36\% \\ \hline
DriveCar & 66.96\% \\ \hline
Eat & 45.45\% \\ \hline
FightPerson & 57.63\% \\ \hline
GetOutCar & 17.86\% \\ \hline
HandShake & 25.93\% \\ \hline
HugPerson & 15.15\% \\ \hline
Kiss & 18.18\% \\ \hline
Run & 38.78\% \\ \hline
SitDown & 40.96\% \\ \hline
SitUp & 5.26\% \\ \hline
StandUp & 35.20\% \\ \hline
Average AP & 31.56\% \\ \hline
%
\end{tabular}
\caption{Average precision for classification using only video features}
\label{table:AP_OnlyAction}
\end{table}


\section{\index{Support Vector Machines}Support Vector Machines}
The BoF representation of all training clips are used to train a
support vector machine (\index{SVM}SVM) and BoF representation of a disjoint set of testing clips is classified using
learnt model. For training a SVM model, a radial basis function (RBF) kernel is used with parameters $C = 100$ 
and $\gamma = 0.01$. A one-versus-rest learning strategy is used to learn as many models as there are action classes.
In case of this data set, there are 12 action classes and thus 12 models are learnt.

Classification using only video features are compared taking \index{Average Average Precision|see {AAP}} average average precision as a measure.
As explained in \cite{actionsInContext}, average precision(AP) approximates area under recall-precision curve.
Thus, AP for each activity class is calculated and then finally averaged over all the classes, average AP (\index{AAP}AAP) is calculated.

The table \ref{table:AP_OnlyAction} shows the APs for each individual class for classification using only video features.

