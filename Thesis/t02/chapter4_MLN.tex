\chapter{Markov Logic Networks}

\label{ch4_MLN}

%Replace \lipsum with text.
% You may have as many sections as you please. This is just for reference.

As explained in \cite{MarkovLogic}, Markov Logic consists of set of First order
logic (FOL) formulae associated with weights. The FOL formulae form the structure 
of Markov Logic Networks (MLNs). Background and theory of MLNs and calculation of
weights of formula is explained in the coming sections.

\section{Markov Networks}
\index{Markov Networks}Markov Network is an undirected graph $G$ with its nodes as set of variables $X = (X_1, X_2,\ldots, X_n) \in \rchi $.
Apart from graph, it also consists of \index{Potential Functions}potential functions $\phi_k$ for each clique $k$ in the graph.
Potential function $\phi_k$ is a real valued function of the state of $k^{th}$ clique.
The joint distribution is given by

\begin{equation}
	\label{jointDist}
	P(X = x) = \frac{1}{Z}{\displaystyle \prod_{k} \phi_{k}(x_{\{k\}})}
\end{equation}

where $x_{\{k\}}$ is the state of $k^{th}$ clique;
$Z$, also known as \index{Partition Function}partition function, is given by

\begin{equation}
	\label{partitionFunc}
	Z = \displaystyle \sum_{x \in \rchi} \displaystyle \prod_{k} \phi_k(x_{\{k\}}) 
\end{equation}

If clique potentials are replaced by exponentiation of weighted sum of features of state,
joint distribution can be given as

\begin{equation}
	\label{jointDistWeighted}
	P(X = x) = \frac{1}{Z} exp \left( \displaystyle \sum_i w_i f_i(x) \right)
\end{equation}

where $w_i$ is the weight of the $i^{th}$ clique and $f_i(x) \in \{0,1\}$ indicates state 
of the $i^{th}$ clique.

\section{Markov Logic}
The domain knowledge is captured by First Order Logic (FOL) formulae. In its basic form, 
first order knowledge base is a set of hard constraints over a set of possible worlds.
Because of these hard constraints, if a world does not satisfy even a single formula,
the world is considered as false or impossible.

Markov Logic uses FOL with soft constrains instead of hard. In markov logic, if a world
does not satisfy a formula $F_i$, it is considered as less probable world in comparison to a
world which satisfies $F_i$ assuming rest of the formulae have same state in both the worlds.

Definition \ref{MLNDef} from \cite{MarkovLogic} formally defines \index{Markov Logic Networks|see {MLN}}
Markov Logic Network (\index{MLN}MLN) as follows

\begin{defn}
	\label{MLNDef}
	A Markov Logic Network(MLN) L is a set of pairs $(F_i, w_i)$, where $F_i$ is a
	formula in first order logic and $w_i$ is weight associated with the formula - a real number.
	Together with a finite set of constants $C = \{c_1, c_2,\ldots,c_{|C|}\}$, MLN $L$
	defines a markov network, $M_{L,C}$ as follows :

	\begin{enumerate}
		\item $M_{L,C}$ contains one binary node for each possible grounding
			of each atom appearing in L. The value of the node is 1 if ground
			atom is true and 0 otherwise.
		\item $M_{L,C}$ contains one feature for each possible grounding of each
			formula $F_i$ in L. The value of the feature is 1 if the ground
			formula is true and 0 otherwise. The weight of the feature is
			$w_i$ associated with $F_i$ in L.
	\end{enumerate}
\end{defn}

Probability distribution over possible world $x$ specified by markov network $M_{L,C}$ is given by
\begin{equation}
	\label{jointDistMLN}
	P(X = x) = \frac{1}{Z} exp \left( \displaystyle \sum_{i = 1}^{F} w_i n_i(x)  \right)
\end{equation}
where $F$ is number of formulae in MLN and $n_i(x)$ is the number of true groundings of
$F_i$ in world $x$.



\section{Use of MLNs in the Project}
Alchemy 2.0 is used to do inference via Markov Logic Networks.
The activity prediction and confidence information from SVM is used
alongwith object detection and confidence information from object detector.

MLN rules look like : \\
\\
\begin{tabular}{l c l}
	\label{MLNRule01}
	$\forall clip$ ActivityConf\_NI\_TO\_N2(clip,activity) & $\implies$ & HasActivity(clip, activity) \\
	$\forall clip$ ActivityConf\_N2\_TO\_N15(clip,activity) & $\implies$ & HasActivity(clip, activity) \\
	$\forall clip$ ActivityConf\_N15\_TO\_N1(clip,activity) & $\implies$ & HasActivity(clip, activity) \\
	~ & $\vdots$ & ~ \\
	$\forall clip$ ActivityConf\_P1\_TO\_P15(clip, activity) & $\implies$ & HasActivity(clip, activity) \\
	$\forall clip$ ActivityConf\_P15\_TO\_P2(clip,activity) & $\implies$ & HasActivity(clip, activity) \\
	$\forall clip$ ActivityConf\_P2\_TO\_PI(clip,activity) & $\implies$ & HasActivity(clip, activity) \\
	~ & $\vdots$ & ~ \\
	$\forall clip$ ObjPresent( clip, chair ) & $\implies$ & HasActivity( clip, Eat ) \\
	$\forall clip$ ObjPresent( clip, chair ) & $\implies$ & HasActivity( clip, SitDown ) \\
	$\forall clip$ ObjPresent( clip, car ) & $\implies$ & HasActivity( clip, DriveCar ) \\
	~ & $\vdots$ & ~ \\
	$\forall clip$ NumPersons\_0\_TO\_1(c) & $\implies$ & HasActivity(c,+a) \\
	~ & $\vdots$ & ~ \\
	$\forall clip$ NumPersons\_2\_TO\_I(c) & $\implies$ & HasActivity(c,+a) \\
\end{tabular}
\\
\\
\\
The output of SVM is binned according to the confidence values.
NI\_TO\_N2 bin corresponds to confidence values from $-\infty$ to $-2$, 
P1\_TO\_P15 corresponds to confidence range of $1$ to $1.5$ ans so on.

Average number of persons per frame is calculated for each clip and
binned similarly.

HasActivity(c, +a) is directive to Alchemy to generate number of rules where +a is replaced
by all possible activities - one rule per activity. In the case of this project, 12 such rules are genrated.



\begin{comment}
\section{Example MLN}
\begin{tikzpicture}
	%[scale=.8,auto=left,every node/.style={circle,fill=blue!20}]
	[every node/.style={oval}]
	\node (n1) at (5,10) {Friends(A,B)};
	\node (n6) at (1,10) {6};
%	\node (n4) at (4,8)  {4};
%	\node (n5) at (8,9)  {5};
%	\node (n1) at (11,8) {1};
%	\node (n2) at (9,6)  {2};
%	\node (n3) at (5,5)  {3};

%	\foreach \from/\to in {n6/n4,n4/n5,n5/n1,n1/n2,n2/n5,n2/n3,n3/n4}
%	\draw (\from) -- (\to);

\end{tikzpicture}
\end{comment}
